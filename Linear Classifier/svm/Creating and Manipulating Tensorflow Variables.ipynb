{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 5 color = #0D0D0D> We use variables to represent the parameters of the model\n",
    "    \n",
    "<font face = \"STCAIYUN\" size = 4 color = #0D0D0D>1. Properties of tensorflow variables:\n",
    "    1. variables must be explicitly initialized before a graph is used for the first time.\n",
    "    2. We can use gradient methods to modify variables after each iterations as we search for the model's optimal parameter.\n",
    "    3. We can store the value in the disk and restore them for later using.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 5 color = #4876FF> Example 1<font face = \"STCAIYUN\" size = 3 color = #4876FF>: Initializing a variable that describe the weight connecting neurons between two layers of a feed-forward neural network\n",
    "    \n",
    "<font face = \"STCAIYUN\" size = 3 color = #4876FF> 例1： 初始化一个作为前递神经网络中连接两层网络的权重\n",
    "    import tensorflow as tf\n",
    "\n",
    "##### Create a variable.\n",
    "w = tf.Variable(<initial-value>, name=<optional-name>)\n",
    "\n",
    "##### Use the variable in the graph like any Tensor.\n",
    "y = tf.matmul(w, ...another variable or tensor...)\n",
    "\n",
    "##### The overloaded operators are available too.\n",
    "z = tf.sigmoid(w + y)\n",
    "\n",
    "##### Assign a new value to the variable with `assign()` or a related method.\n",
    "w.assign(w + 1.0)\n",
    "w.assign_add(1.0)\n",
    "##### Explain the code below\n",
    "<font face = \"Times\" size = 4 color = #CD0000 > tf.random_normal <font face = \"STCAIYUN\" size = 3 color = #4876FF> produces a tensor initialized using a normal distribution with standard deviation 0.5. And specified the tensor size of 300*200, which means the weight connect a layer with 300 neurons to a layer with 200 neurons. Also passed a name to this variable, which is a unique identifier that allows us to refer the appropriate node in the computation graph. \n",
    "    \n",
    "<font face = \"STCAIYUN\" size = 3 color = #4876FF > In this case, \"Weights\" is meant to be <font face = \"Times\" size = 4 color = #CD0000 >trainable. <font face = \"STCAIYUN\" size = 3 color = #4876FF >In another words, the computer would automatically apply gradients to weights. If \"Weights\" is not trainable, we will pass an optional flag when calling <font face = \"Times\" size = 4 color = #CD0000 >tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "weights = tf. Variable(tf.random_normal([300,200], stddev = 0.5), name=\"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 3 color = #4876FF > If \"Weights\" is not trainable, we will pass an optional flag when calling <font face = \"Times\" size = 4 color = #CD0000 >tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([300,200],stddev = 0.5),name=\"weights\", trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 4 color = #0D0D0D> Other methods to initialize a tensorflow variable\n",
    "\n",
    "    1. w1 = tf.Variable(tf.zeros([10,20], dtype=tf.float32),name = \"w1\")\n",
    "\n",
    "    2. w2 = tf.Variable(tf.ones([10,20], dtype=tf.float32),name = \"w2\")\n",
    "\n",
    "    3. tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "\n",
    "    4. tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "\n",
    "    5. tf.random_uniform(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 5 color = #0D0D0D>2. Tensorflow operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 3 color = #0D0D0D> 1.Tensorflow operations represent abstract transformations that are applied to tensors in the computation graph.\n",
    "    \n",
    "<font face = \"STCAIYUN\" size = 3 color = #0D0D0D> 2.An operation consists of one or more <font face = \"Times\" size = 4 color = #CD0000 > kernels<font face = \"STCAIYUN\" size = 3 color = #0D0D0D>, which represent device-specific implementations.\n",
    "\n",
    "<font face = \"STCAIYUN\" size = 3 color = #0D0D0D>    Category:\n",
    "    1. Element-wise mathmatical operations:   add, sub, mul\n",
    "    2. Array opeartion:                       Concat, Slice, Split, COnstant\n",
    "    3. Matrix operation:                      MatMul, MatrixInverse\n",
    "    4. Stateful operation:                    Variable, Assign, AssignAdd...\n",
    "    5. Neural Network Building blocks:        SoftMax, Sigmoid\n",
    "    6. Checkpointing Opeartions:              Save, Restore\n",
    "    7. Queue and Synchronization operations:  Enqueue, Dequeue,\n",
    "    8. Control flow operations:               Merge, Switch, Enter, Leave,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 5 color = #0D0D0D>3. Placeholder Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 3 color = #0D0D0D> 1. A variable is insufficient because it's only meant to be initialized once. Instead, we need a component that we populate every single time the computation graph is run.\n",
    "    \n",
    "<font face = \"STCAIYUN\" size = 3 color = #0D0D0D> 2. Tensorflow using a <font face = \"STCAIYUN\" size = 3 color = #CD0000> placeholder<font face = \"STCAIYUN\" size = 3 color = #0D0D0D>, which is instantiated as follows and can be used in operatios just like ordinary Tensorflow variables and tensors.\n",
    "    placeholder，中文意思是占位符，在tensorflow中类似于函数参数，运行时必须传入值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[263.8907  250.83559 254.67218 ... 261.70337 250.21956 253.78177]\n",
      " [260.59814 252.50601 252.23154 ... 258.90118 247.68295 257.3816 ]\n",
      " [266.1075  250.12056 252.49786 ... 259.2127  257.20966 258.8991 ]\n",
      " ...\n",
      " [270.45453 263.57144 262.1198  ... 267.57272 265.24194 262.7698 ]\n",
      " [264.0088  251.87402 254.65886 ... 262.8594  252.54123 257.56552]\n",
      " [266.47668 251.5961  253.57938 ... 262.3086  253.12541 256.22766]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = tf.placeholder(tf.float32, shape=(1024, 1024))\n",
    "y = tf.matmul(x, x)\n",
    "with tf.Session() as sess:\n",
    "  #print(sess.run(y))  # ERROR: will fail because x was not fed.\n",
    "  rand_array = np.random.rand(1024, 1024)\n",
    "  print(sess.run(y, feed_dict={x: rand_array}))  # Will succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 定义placeholder\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "# 定义乘法运算\n",
    "output = tf.multiply(input1, input2)\n",
    "# 通过session执行乘法运行\n",
    "with tf.Session() as sess:\n",
    "    # 执行时要传入placeholder的值\n",
    " print(sess.run(output, feed_dict = {input1:[7.], input2: [2.]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 5 color = #0D0D0D>4.Sessions in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 3 color = #0D0D0D>1.A Tensorflow program interacts with a computation graph using a <font face = \"STCAIYUN\" size = 3 color = #CD0000> session\n",
    "    \n",
    "<font face = \"STCAIYUN\" size = 3 color = #0D0D0D>2. The Tensorflow session is responsible for building the initial graph, and can be used to initialized all variables appropriately and to run the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "weight = tf.get_variable(\"weight\", initializer=tf.constant([0.1]))\n",
    "bias = tf.get_variable(\"bias\", initializer=tf.constant([0.0]))\n",
    "\n",
    "# Placeholder for input and prediction\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Define a linear model y = Wx + b\n",
    "model = weight * x + bias\n",
    "\n",
    "#loss = tf.reduce_sum(tf.square(model - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 cost: 10.562399864196777\n",
      "i: 100 cost: 0.06185010448098183\n",
      "i: 200 cost: 0.005557416472584009\n",
      "i: 300 cost: 0.0004993483889847994\n",
      "i: 400 cost: 4.486760008148849e-05\n",
      "i: 500 cost: 4.031036041851621e-06\n",
      "i: 600 cost: 3.6235860534361564e-07\n",
      "i: 700 cost: 3.264136694269837e-08\n",
      "i: 800 cost: 2.924366526713129e-09\n",
      "i: 900 cost: 2.610960336824064e-10\n",
      "i: 1000 cost: 2.2751578399038408e-11\n",
      "i: 1100 cost: 2.5579538487363607e-12\n",
      "i: 1200 cost: 2.5579538487363607e-12\n",
      "i: 1300 cost: 2.5579538487363607e-12\n",
      "i: 1400 cost: 2.5579538487363607e-12\n",
      "i: 1500 cost: 2.5579538487363607e-12\n",
      "i: 1600 cost: 2.5579538487363607e-12\n",
      "i: 1700 cost: 2.5579538487363607e-12\n",
      "i: 1800 cost: 2.5579538487363607e-12\n",
      "i: 1900 cost: 2.5579538487363607e-12\n",
      "W: [1.9999993] b: [-0.49999803] cost: 2.5579538487363607e-12\n"
     ]
    }
   ],
   "source": [
    "### Define a cost function, an optimizer and a trainer\n",
    "# Define a cost function (Mean square error - MSE)\n",
    "loss = tf.reduce_sum(tf.square(model - y))\n",
    "\n",
    "# Optimizer with a 0.01 learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "### Training (Fitting)\n",
    "# Training data\n",
    "x_train = [1.0, 2.0, 3.0, 4.0]\n",
    "y_train = [1.5, 3.5, 5.5, 7.5]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Retrieve the variable initializer op and initialize variable W & b.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        sess.run(train, {x:x_train, y:y_train})\n",
    "        if i%100==0:\n",
    "            l_cost = sess.run(loss, {x:x_train, y:y_train})\n",
    "            print(f\"i: {i} cost: {l_cost}\")\n",
    "\n",
    "    # Evaluate training accuracy\n",
    "    l_W, l_b, l_cost  = sess.run([weight, bias, loss], {x:x_train, y:y_train})\n",
    "    print(f\"W: {l_W} b: {l_b} cost: {l_cost}\")\n",
    "    # W: [ 1.99999797] b: [-0.49999401] cost: 2.2751578399038408e-11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 5 color = #0D0D0D>5. Nagigating Variable Scopes and Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D> 1. Building a complex model requires reusing and sharing large set of variables that we'll want to instantiate together in one place.\n",
    "    \n",
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D> 2. Example: This network setup consist of six variables describing 3 layers. If we want to reuse this network multiple times, we prefer to encapsulate it into a compact function like<font face = \"STCAIYUN\" size = 3 color = #CD0000> my_network\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_network(input):\n",
    "    weight_1 = tf.Variable(tf.random_uniform([784,100],-1,1),name=\"weight_1\")\n",
    "    bias_1 = tf.Variable(tf.zeros([100]),name=\"bias_1\")\n",
    "    out1 = tf.matmul(input, weight_1)+bias_1\n",
    "    \n",
    "    weight_2 = tf.Variable(tf.random_uniform([100,50],-1,1),name=\"weight_2\")\n",
    "    bias_2 = tf.Variable(tf.zeros([50]),name=\"bias_2\")\n",
    "    out2 = tf.matmul(out1, weight_2)+bias_2\n",
    "    \n",
    "    weight_3 = tf.Variable(tf.random_uniform([50,10],-1,1),name=\"weight_3\")\n",
    "    bias_3 = tf.Variable(tf.zeros([10]),name=\"bias_3\")\n",
    "    out3 = tf.matmul(out2, weight_3)+bias_3\n",
    "    \n",
    "    #Print names\n",
    "    print(\"Print name of weight parameters\")\n",
    "    print(weight_1.name, weight_2.name, weight_3.name)\n",
    "    print(\"Print name of bias pararmeters\")\n",
    "    print(bias_1.name, bias_2.name, bias_3.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D> The second call to<font face = \"STCAIYUN\" size = 3 color = #CD0000> my_network<font face=\"STCAIYUN\" size = 3 color = #0D0D0D> doesn't use the same variables as the frist call. Instead, we've created a second set of variable.在许多情况下，我们不想创建副本，而是重用模型及其变量。所以需要使用更先进的命名方案来更好的利用tensorflow's variable scoping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print name of weight parameters\n",
      "weight_1_7:0 weight_2_7:0 weight_3_7:0\n",
      "Print name of bias pararmeters\n",
      "bias_1_7:0 bias_2_7:0 bias_3_7:0\n",
      "\n",
      "Print name of weight parameters\n",
      "weight_1_8:0 weight_2_8:0 weight_3_8:0\n",
      "Print name of bias pararmeters\n",
      "bias_1_8:0 bias_2_8:0 bias_3_8:0\n"
     ]
    }
   ],
   "source": [
    "i_1 = tf.placeholder(tf.float32, [1000,784],name = \"i_1\")\n",
    "my_network(i_1)\n",
    "print()\n",
    "i_2 = tf.placeholder(tf.float32, [1000,784],name = \"i_2\")\n",
    "my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D> Tensorflow的变量范围机制是由以下两个function控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"STCAIYUN\" size = 3 color = #FF0000> tf.get_variable(name, shape, initializer)\n",
    "    \n",
    "<font face=\"STCAIYUN\" size = 3 color = #0000CC> Check if a variable with this name exists, retrieves the variable if it does, or creates it using the shape and initializer if it doesn't\n",
    "    检查是否存在具有此名称的变量，如果存在，则检索变量，或者如果不存在则使用形状和初始化程序创建变量\n",
    "\n",
    "<font face=\"STCAIYUN\" size = 3 color = #FF0000> tf.get_variable_scope(scope_name)\n",
    "    \n",
    "<font face=\"STCAIYUN\" size = 3 color = #0000CC> Manage the namespace and determines the scope in which tf.get_variable operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D> 3. Rewrite my_network in a cleaner fashion using Tensorflow variable scoping. The new name  of our variables are namespaced as<font face = \"STCAIYUN\" size = 3 color = #CD0000> \"layer1/w\", \"layer2/b\"...\n",
    "    \n",
    "    \n",
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D>    * tf.name_scope() 并不会对 tf.get_variable() 创建的变量有任何影响。\n",
    "\n",
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D>    * tf.name_scope() 主要是用来管理命名空间的，这样子让我们的整个模型更加有条理。而 tf.variable_scope() 的作用是为了实现变量共享，它和 tf.get_variable() 来完成变量共享的功能\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_3/add:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    #定义如何初始化权重和bias\n",
    "    weight_init = tf. random_uniform_initializer(minval=-1, maxval=1)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    #定义如何初始化具有“W”，“b”这两个名字的变量，通过以上两个来进行初始化\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    \n",
    "    return tf.matmul(input, W)+b\n",
    "\n",
    "def my_network(input):\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        output_1 = layer(input, [784,100], [100])\n",
    "        \n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        output_2 = layer(output_1, [100,50], [50])\n",
    "    \n",
    "    with tf.variable_scope(\"layer_3\"):\n",
    "        output_3 = layer(output_2, [50,10], [10])\n",
    "    \n",
    "    return output_3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "i_1 = tf.placeholder(tf.float32, [1000,784],name = \"i_1\")\n",
    "my_network(i_1)\n",
    "tf.reset_default_graph()\n",
    "i_2 = tf.placeholder(tf.float32, [1000,784],name = \"i_2\")\n",
    "my_network(i_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder_4:0\n",
      "ph_8:0\n",
      "ph_9:0\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"ph_9:0\", shape=(2, 3, 4), dtype=float32)\n",
      "\n",
      "Variable_4:0\n",
      "V_8:0\n",
      "V_9:0\n",
      "<class 'tensorflow.python.ops.variables.RefVariable'>\n",
      "<tf.Variable 'V_9:0' shape=(2,) dtype=float32_ref>\n",
      "\n",
      "gv2:0\n",
      "gv1:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# 1.placeholder \n",
    "v1 = tf.placeholder(tf.float32, shape=[2,3,4])\n",
    "print (v1.name)\n",
    "v1 = tf.placeholder(tf.float32, shape=[2,3,4], name='ph')\n",
    "print (v1.name)\n",
    "v1 = tf.placeholder(tf.float32, shape=[2,3,4], name='ph')\n",
    "print (v1.name)\n",
    "print (type(v1))\n",
    "print (v1)\n",
    "print()\n",
    "# 2. tf.Variable()\n",
    "v2 = tf.Variable([1,2], dtype=tf.float32)\n",
    "print (v2.name)\n",
    "v2 = tf.Variable([1,2], dtype=tf.float32, name='V')\n",
    "print (v2.name)\n",
    "v2 = tf.Variable([1,2], dtype=tf.float32, name='V')\n",
    "print (v2.name)\n",
    "print (type(v2))\n",
    "print (v2)\n",
    "print()\n",
    "tf.reset_default_graph()\n",
    "# 3.tf.get_variable() 创建变量的时候必须要提供 name\n",
    "v3 = tf.get_variable(name='gv2', shape=[])  \n",
    "print (v3.name)\n",
    "v4 = tf.get_variable(name='gv1', shape=[2])\n",
    "print (v4.name)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.name:  nsc1/v1:0\n",
      "v2.name:  nsc1/vsc1/v2:0\n",
      "v3.name:  vsc1/v3:0\n",
      "v4.name:  nsc1_1/v4:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope('nsc1'):\n",
    "    v1 = tf.Variable([1], name='v1')\n",
    "    with tf.variable_scope('vsc1'):\n",
    "        v2 = tf.Variable([1], name='v2')\n",
    "        v3 = tf.get_variable(name='v3', shape=[])\n",
    "print ('v1.name: ', v1.name)\n",
    "print ('v2.name: ', v2.name)\n",
    "print ('v3.name: ', v3.name)\n",
    "with tf.name_scope('nsc1'):\n",
    "    v4 = tf.Variable([1], name='v4')\n",
    "print ('v4.name: ', v4.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"STCAIYUN\" size = 3 color = #0D0D0D> 1. 查看命名空间和使用场景\n",
    "    \n",
    "    第一组，用 tf.Variable() 的方式来定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 train_able_variables in the Graph: \n",
      "<tf.Variable 'conv1_weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv1_biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv1_weights_1:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv1_biases_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_weights_1:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_biases_1:0' shape=(32,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# 拿官方的例子改动一下\n",
    "def my_image_filter():\n",
    "    conv1_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")\n",
    "    return None\n",
    "\n",
    "# First call creates one set of 4 variables.\n",
    "result1 = my_image_filter()\n",
    "# Another set of 4 variables is created in the second call.\n",
    "result2 = my_image_filter()\n",
    "# 获取所有的可训练变量\n",
    "vs = tf.trainable_variables()\n",
    "print ('There are %d train_able_variables in the Graph: ' % len(vs))\n",
    "for v in vs:\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 train_able_variables in the Graph: \n",
      "<tf.Variable 'image_filters/conv1/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'image_filters/conv1/biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'image_filters/conv2/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'image_filters/conv2/biases:0' shape=(32,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 设置GPU按需增长\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# 下面是定义一个卷积层的通用方式\n",
    "def conv_relu(kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    return None\n",
    "\n",
    "\n",
    "def my_image_filter():\n",
    "    # 按照下面的方式定义卷积层，非常直观，而且富有层次感\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu([5, 5, 32, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu( [5, 5, 32, 32], [32])\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"image_filters\") as scope:\n",
    "    # 下面我们两次调用 my_image_filter 函数，但是由于引入了 变量共享机制\n",
    "    # 可以看到我们只是创建了一遍网络结构。\n",
    "    result1 = my_image_filter()\n",
    "    scope.reuse_variables()\n",
    "    result2 = my_image_filter()\n",
    "\n",
    "\n",
    "# 看看下面，完美地实现了变量共享！！！\n",
    "vs = tf.trainable_variables()\n",
    "print ('There are %d train_able_variables in the Graph: ' % len(vs))\n",
    "for v in vs:\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face = \"STCAIYUN\" size = 5 color = #0D0D0D>6. Managing Models over the CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
